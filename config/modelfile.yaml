version: 1.0
metadata:
  author: "Deval Shah"
  last_modified: "2024-06-07"
  description: "Prompt to generate answer based on the query and context"
model:
  name: "llama3:8b"
  # format: "json"
  options:
    temperature: 0.3 # controlling the randomness of the responses.
    num_predict: 256
    num_ctx: 2048
    #top_k: 40 # Reduces the probability of generating nonsense.
    #top_p: 0.9
  stream: false
  raw: true
  keep_alive: "5m" # Configures how long the model remains loaded in memory after a request.
  system: "You are an assistant for question-answering tasks" #"system message to (overrides what is defined in the Modelfile)"
  #context: the context parameter returned from a previous request to /generate, this can be used to keep a short conversational memory
prompts:
  - role: user
    text: >
      Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
    type: text