app:
  name: "enterprise-search"
  enabled: true
  image:
    repository: docker.aiml.team/products/aiml/enterprise-search/llamasearch
    tag: latest
    pullPolicy: Always
  service:
    type: ClusterIP
    port: 8000
    targetPort: 8000
  resources:
    limits:
      nvidia.com/gpu: "1"
      cpu: "10000m"
      memory: "10Gi"
  env:
    CONFIG_PATH: "/app/config.yaml"
    OLLAMA_SERVER_URL: "http://ollama-es-service.aiml-engineering.svc.cluster.local:80"
  volumes:
    config:
      configMapName: "es-cmap"
    data:
      pvcName: "es-pvc"
  gpuProduct: "NVIDIA-RTX-6000-Ada-Generation"
  # gpuProduct: "NVIDIA-L40S"

ollama:
  name: ollama-es
  enabled: true
  namespace: aiml-engineering
  image:
    repository: ollama/ollama
    tag: latest
    pullPolicy: Always
  service:
    type: ClusterIP
    port: 80
    targetPort: 11434
  portName: http
  resources:
    limits:
      nvidia.com/gpu: "1"
      cpu: "10000m"
      memory: "40Gi"
  gpuProduct: "NVIDIA-L40S"
  #gpuProduct: "NVIDIA-RTX-6000-Ada-Generation"
  #gpuProduct: "NVIDIA-A100-SXM4-40GB"
  env:
    OLLAMA_HOST: "0.0.0.0"
  volumes:
    data:
      pvcName: "es-pvc"
  gpu:
    # -- Enable GPU integration
    enabled: true
    # -- GPU type: 'nvidia' or 'amd'
    # If 'ollama.gpu.enabled', default value is nvidia
    # If set to 'amd', this will add 'rocm' suffix to image tag if 'image.tag' is not override
    # This is due cause AMD and CPU/CUDA are different images
    type: 'nvidia'
    # -- Specify the number of GPU
    number: 1
  # -- List of models to pull at container startup
  # The more you add, the longer the container will take to start if models are not present
  models:
   - mistral:7b-instruct
   - llama3:8b
   - llama3:instruct
   - llama3:70b
   - llama3-chatqa:70b
  #  - mixtral
  insecure: true
  livenessProbe:
    enabled: true
    path: /
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  # Configure extra options for readiness probe
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  readinessProbe:
    enabled: true
    path: /
    initialDelaySeconds: 30
    periodSeconds: 30
    timeoutSeconds: 3
    failureThreshold: 6
    successThreshold: 1

qdrant:
  enabled: true
  resources:
    limits:
      # nvidia.com/gpu: "1"
      cpu: "5000m"
      memory: "15Gi"
  image:
    repository: qdrant/qdrant
    tag: latest
  service:
    port: 6333
  collectionName: test
  vectorSize: 384
  distance: Cosine
  volumes:
    config:
      configMapName: "es-cmap"
    script:
      configMapName: "create-collection-script"
  gpuProduct: "NVIDIA-RTX-6000-Ada-Generation"
  env:
    CONFIG_PATH: "/app/config.yaml"

redis:
  enabled: true
  resources:
    limits:
      cpu: "5000m"
      memory: "15Gi"
  image:
    repository: redis
    tag: latest
  service:
    port: 6379
  gpuProduct: "NVIDIA-RTX-6000-Ada-Generation"

prometheus:
  enabled: true
  image:
    repository: prom/prometheus
    tag: v2.26.0
    pullPolicy: IfNotPresent
  service:
    type: NodePort
    port: 9090
    nodePort: 30090
  resources:
    requests:
      memory: "512Mi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1"
  storage:
    useEmptyDir: true
  volumes:
    config:
      configMapName: "es-cmap"
    data:
      pvcName: "es-pvc"
  gpuProduct: "NVIDIA-RTX-6000-Ada-Generation"
