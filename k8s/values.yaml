app:
  name: "enterprise-search"
  namespace: aiml-engineering
  enabled: true
  image:
    repository: docker.aiml.team/products/aiml/enterprise-search/llamasearch
    tag: latest
    pullPolicy: Always
  service:
    type: ClusterIP
    port: 8010
    targetPort: 8010
  resources:
    limits:
      nvidia.com/gpu: "1"
      cpu: "10000m"
      memory: "10Gi"
  env:
    # Pipeline Settings
    APP_BASE_PATH: /app
    CONFIG_PATH: "/app/config.yaml"
    DATA_PATH: "/data/files"
    LOG_DIR: "/data/app/logs"
    OLLAMA_SERVER_URL: "http://ollama-es-service.aiml-engineering.svc.cluster.local:80"
    OPENAI_API_KEY: "" # Required if you are using the OpenAI models and `use_openai` flag is enabled (llm/embedding)
    DOCKER_ENV: true
    #API Settings
    API_V1_STR: "/api/v1"
    SERVER_NAME: "EnterpriseSearch"
    SERVER_HOST: "http://localhost:8010"
    PROJECT_NAME: "EnterpriseSearch"
    ENABLE_RATE_LIMIT: false
    BACKEND_CORS_ORIGINS: "http://localhost:3000,http://localhost:3001,http://localhost:3002"
    DATABASE_URL: "sqlite+aiosqlite:///./test.db"
    REDIS_URL: "redis://redis-service:6379/0"
    ENABLE_AUTH: true
    COOKIE_SECURE: false
    LOGLEVEL: "DEBUG"
    FIREBASE_CREDENTIALS_PATH: "/app/keys/firebase.json"

  volumes:
    config:
      configMapName: "es-cmap"
    data:
      pvcName: "es-pvc"
    firebase:
      secretName: "firebase-credentials"
  gpuProduct: "NVIDIA-RTX-6000-Ada-Generation"
  # gpuProduct: "NVIDIA-L40S"

ollama:
  name: ollama-es
  enabled: true
  namespace: aiml-engineering
  image:
    repository: ollama/ollama
    tag: 0.3.6
    pullPolicy: Always
  service:
    type: ClusterIP
    port: 80
    targetPort: 11434
  portName: http
  resources:
    limits:
      nvidia.com/gpu: "1"
      cpu: "10000m"
      memory: "40Gi"
  # gpuProduct: "NVIDIA-L40S"
  env:
    OLLAMA_HOST: "0.0.0.0"
  volumes:
    data:
      pvcName: "es-pvc"
  gpu:
    # -- Enable GPU integration
    enabled: true
    # -- GPU type: 'nvidia' or 'amd'
    # If 'ollama.gpu.enabled', default value is nvidia
    # If set to 'amd', this will add 'rocm' suffix to image tag if 'image.tag' is not override
    # This is due cause AMD and CPU/CUDA are different images
    type: 'nvidia'
    # -- Specify the number of GPU
    number: 1
  # -- List of models to pull at container startup
  # The more you add, the longer the container will take to start if models are not present
  models:
    - ajindal/llama3.1-storm:8b
  #  - llama3:instruct
  #  - llama3:70b
  insecure: true
  livenessProbe:
    enabled: true
    path: /
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  # Configure extra options for readiness probe
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  readinessProbe:
    enabled: true
    path: /
    initialDelaySeconds: 30
    periodSeconds: 30
    timeoutSeconds: 3
    failureThreshold: 6
    successThreshold: 1

qdrant:
  namespace: aiml-engineering
  enabled: true
  resources:
    limits:
      # nvidia.com/gpu: "1"
      cpu: "5000m"
      memory: "15Gi"
  image:
    repository: qdrant/qdrant
    tag: latest
  service:
    port: 6333
  collectionName: test
  vectorSize: 384
  distance: Cosine
  volumes:
    config:
      configMapName: "es-cmap"
  gpuProduct: "NVIDIA-RTX-6000-Ada-Generation"
  env:
    CONFIG_PATH: "/app/config.yaml"

redis:
  namespace: aiml-engineering
  enabled: true
  resources:
    limits:
      cpu: "5000m"
      memory: "15Gi"
  image:
    repository: redis
    tag: latest
  service:
    port: 6379
  gpuProduct: "NVIDIA-RTX-6000-Ada-Generation"

# prometheus:
#   namespace: aiml-engineering
#   enabled: true
#   image:
#     repository: prom/prometheus
#     tag: v2.26.0
#     pullPolicy: IfNotPresent
#   service:
#     type: NodePort
#     port: 9090
#     nodePort: 30090
#   resources:
#     requests:
#       memory: "512Mi"
#       cpu: "500m"
#     limits:
#       memory: "2Gi"
#       cpu: "1"
#   storage:
#     useEmptyDir: true
#   volumes:
#     config:
#       configMapName: "es-cmap"
#     data:
#       pvcName: "es-pvc"
#   gpuProduct: "NVIDIA-RTX-6000-Ada-Generation"
