{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72f0f68e-30d9-4be6-b905-c12837b5c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enterprise Search Cookbook\n",
    "\n",
    "## Enterprise Search is a foundation for building Retrieval-Augmented Generation (RAG) pipelines. It provides accurate answers based on your documents and offers a simple API for indexing and querying document collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60671cf7-976a-4466-b1be-066ba8353ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Setup and Prerequisites\n",
    "\n",
    "# First, let's set up our environment:\n",
    "# Ensure you have conda installed in your system before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1955dc6-ab26-4338-9bb1-2774be248dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping qdrant_vector_store ... \n",
      "Stopping redis_doc_store     ... \n",
      "\u001b[1B\u001b[33mWARNING\u001b[0m: Found orphan containers (ollama) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.\n",
      "Removing qdrant_vector_store ... \n",
      "Removing redis_doc_store     ... \n",
      "\u001b[2BRemoving network docker_default32mdone\u001b[0m\n",
      "Creating network \"docker_default\" with the default driver\n",
      "\u001b[33mWARNING\u001b[0m: Found orphan containers (ollama) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.\n",
      "Creating qdrant_vector_store ... \n",
      "Creating redis_doc_store     ... \n",
      "\u001b[2Bting qdrant_vector_store ... \u001b[32mdone\u001b[0m\u001b[2A\u001b[2K\u001b[33mWARNING\u001b[0m: Found orphan containers (qdrant_vector_store, redis_doc_store) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.\n",
      "ollama is up-to-date\n"
     ]
    }
   ],
   "source": [
    "## 3. Configuration\n",
    "\n",
    "# Let's set up the configuration:\n",
    "\n",
    "# !cp .env.example .env\n",
    "\n",
    "# Edit .env file with your settings\n",
    "# You can use !echo \"KEY=VALUE\" >> .env to add settings\n",
    "\n",
    "# Start Redis and Qdrant services\n",
    "!docker-compose -f docker/docker-compose.yml down\n",
    "!docker-compose -f docker/docker-compose.yml up -d redis qdrant\n",
    "\n",
    "# Setup LLM (Ollama example)\n",
    "!docker-compose -f docker/docker-compose-ollama.yml up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bfd15aa-9177-432f-883a-e145dc705a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deval/Document/Work/miniconda3/envs/llamasearch_merge_test/lib/python3.9/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_name\" in Eval has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Path Configuration:\n",
      "================================================================================\n",
      "BASE_PATH:     /home/deval/Documents/Work/Deval/ES/LlamaSearch_final/LlamaSearch\n",
      "--------------------------------------------------------------------------------\n",
      "Resolved Paths:\n",
      "Config File:   /home/deval/Documents/Work/Deval/ES/LlamaSearch_final/LlamaSearch/config/config.dev.yaml\n",
      "Data Path:     /home/deval/Documents/Work/Deval/ES/LlamaSearch_final/LlamaSearch/data/test_docs\n",
      "Log Directory: /home/deval/Documents/Work/Deval/ES/LlamaSearch_final/LlamaSearch/data/app/logs\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 15:37:33,180 - \u001b[32m[INFO]\u001b[0m - \u001b[37mUsing embedding model: Alibaba-NLP/gte-Qwen2-1.5B-instruct\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44d41f56daa4641bc42cddb43e0f4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-10-15 15:37:40,743 - \u001b[32m[INFO]\u001b[0m - \u001b[37mRunning model ajindal/llama3.1-storm:8b on http://localhost:11434\u001b[0m\n",
      "2024-10-15 15:37:40,744 - \u001b[32m[INFO]\u001b[0m - \u001b[37mMulti tenancy: True\u001b[0m\n",
      "2024-10-15 15:37:40,744 - \u001b[32m[INFO]\u001b[0m - \u001b[37mSetting up Qdrant index...\u001b[0m\n",
      "2024-10-15 15:37:40,773 - \u001b[36m[DEBUG]\u001b[0m - \u001b[36mCollection names: []\u001b[0m\n",
      "2024-10-15 15:37:40,774 - \u001b[32m[INFO]\u001b[0m - \u001b[37mCollection my_test_collection does not exist. Creating new collection...\u001b[0m\n",
      "Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store for collection my_test_collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 15:37:42,366 - \u001b[32m[INFO]\u001b[0m - \u001b[37mQdrant index setup completed.\u001b[0m\n",
      "2024-10-15 15:37:42,368 - \u001b[32m[INFO]\u001b[0m - \u001b[37mSetting up Docstore...\u001b[0m\n",
      "2024-10-15 15:37:42,370 - \u001b[32m[INFO]\u001b[0m - \u001b[37mDocstore setup completed.\u001b[0m\n",
      "2024-10-15 15:37:42,372 - \u001b[32m[INFO]\u001b[0m - \u001b[37mSetting up Parser...\u001b[0m\n",
      "2024-10-15 15:37:42,571 - \u001b[32m[INFO]\u001b[0m - \u001b[37mParser setup completed.\u001b[0m\n",
      "2024-10-15 15:37:42,572 - \u001b[32m[INFO]\u001b[0m - \u001b[37mSetting up Documents...\u001b[0m\n",
      "2024-10-15 15:37:43,586 - \u001b[32m[INFO]\u001b[0m - \u001b[37mDocuments setup completed.\u001b[0m\n",
      "2024-10-15 15:37:43,586 - \u001b[32m[INFO]\u001b[0m - \u001b[37mSetting up Index creation...\u001b[0m\n",
      "2024-10-15 15:37:43,587 - \u001b[32m[INFO]\u001b[0m - \u001b[37mIndex creation setup completed.\u001b[0m\n",
      "2024-10-15 15:37:43,587 - \u001b[32m[INFO]\u001b[0m - \u001b[37mSetting up Ingestion pipeline...\u001b[0m\n",
      "2024-10-15 15:37:43,615 - \u001b[32m[INFO]\u001b[0m - \u001b[37mIngesting 43 nodes for 38 chunks\u001b[0m\n",
      "2024-10-15 15:37:43,615 - \u001b[32m[INFO]\u001b[0m - \u001b[37mAdding nodes to index...\u001b[0m\n",
      "Generating embeddings: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:06<00:00,  1.20s/it]\n",
      "2024-10-15 15:37:50,062 - \u001b[32m[INFO]\u001b[0m - \u001b[37mIngestion pipeline setup completed.\u001b[0m\n",
      "2024-10-15 15:37:50,062 - \u001b[32m[INFO]\u001b[0m - \u001b[37mIngestion pipeline setup completed.\u001b[0m\n",
      "2024-10-15 15:37:50,063 - \u001b[32m[INFO]\u001b[0m - \u001b[37mSetting up Query engine...\u001b[0m\n",
      "2024-10-15 15:37:50,063 - \u001b[36m[DEBUG]\u001b[0m - \u001b[36mMulti tenancy enabled for query engine...\u001b[0m\n",
      "2024-10-15 15:37:50,063 - \u001b[36m[DEBUG]\u001b[0m - \u001b[36mHybrid search is enabled...\u001b[0m\n",
      "2024-10-15 15:37:50,064 - \u001b[32m[INFO]\u001b[0m - \u001b[37mQuery engine setup completed.\u001b[0m\n",
      "2024-10-15 15:37:50,067 - \u001b[32m[INFO]\u001b[0m - \u001b[37mIngesting 0 nodes for 38 chunks\u001b[0m\n",
      "2024-10-15 15:37:50,067 - \u001b[32m[INFO]\u001b[0m - \u001b[37mAdding nodes to index...\u001b[0m\n",
      "2024-10-15 15:37:50,068 - \u001b[32m[INFO]\u001b[0m - \u001b[37mIngestion pipeline setup completed.\u001b[0m\n",
      "2024-10-15 15:37:50,068 - \u001b[32m[INFO]\u001b[0m - \u001b[37mAll setup steps completed successfully.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline setup complete. Ready for queries!\n"
     ]
    }
   ],
   "source": [
    "## 4. Configuring and Running the Pipeline\n",
    "\n",
    "# First, let's update some important configuration settings\n",
    "from llamasearch.settings import config\n",
    "from llamasearch.pipeline import Pipeline, setup_global_embed_model\n",
    "\n",
    "# Override important settings\n",
    "config.application.data_path = \"./data/test_docs/\"  # Path to your documents\n",
    "config.vector_store_config.collection_name = \"my_test_collection\"\n",
    "# config.embedding.model = \"sentence-transformers/all-MiniLM-L6-v2\"  # A smaller, faster model for testing\n",
    "# config.vector_store_config.vector_size = 384 # Update vector dims to match the embed model dims\n",
    "\n",
    "# Set up the global embedding model\n",
    "global_embed_model = setup_global_embed_model(config)\n",
    "\n",
    "# Now, let's run the pipeline:\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "async def run_pipeline():\n",
    "    tenant_id = \"test_tenant\"\n",
    "    pipeline = Pipeline(config, tenant_id, global_embed_model)\n",
    "    await pipeline.setup()\n",
    "    return pipeline\n",
    "\n",
    "# Use this for Jupyter notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "pipeline = asyncio.get_event_loop().run_until_complete(run_pipeline())\n",
    "\n",
    "print(\"Pipeline setup complete. Ready for queries!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef00c31-ce18-433e-b3b4-fe93a72bb568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How does attention mechanism work in transformer architecture?\n",
      "Response: The attention mechanism works by computing a weighted sum of values based on a compatibility function between a query and a set of key-value pairs. In the Transformer architecture, this is done using scaled dot-product attention, where the input consists of queries and keys of dimension dk, and values of dimension dv. The attention function is computed as softmax(QKT√dk)V, where Q, K, and V are matrices of queries, keys, and values, respectively.\n"
     ]
    }
   ],
   "source": [
    "## 5. Querying and Results\n",
    "\n",
    "# Let's perform a query:\n",
    "\n",
    "async def perform_query(pipeline, query):\n",
    "    response = await pipeline.perform_query_async(query)\n",
    "    return response\n",
    "\n",
    "query = \"How does attention mechanism work in transformer architecture?\"\n",
    "response = asyncio.run(perform_query(pipeline, query))\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bdedb44-6840-4240-ac02-53762cb51e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document Information ---\n",
      "+-------------------------------+-----------------+--------------------------------------+\n",
      "| File Name                     | Last Modified   | Doc ID                               |\n",
      "+===============================+=================+======================================+\n",
      "| attention_is_all_you_need.pdf | 2024-10-04      | aa1f72b2-3b1d-444f-b367-adb17b8fb29b |\n",
      "+-------------------------------+-----------------+--------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Pretty print the context\n",
    "pipeline.pretty_print_context(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b423aa-306c-4729-ad86-8b4793a1bd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline cleaned up.\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "asyncio.run(pipeline.cleanup())\n",
    "\n",
    "print(\"Pipeline cleaned up.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
