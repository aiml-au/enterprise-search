{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f0f68e-30d9-4be6-b905-c12837b5c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enterprise Search Cookbook\n",
    "\n",
    "## Enterprise Search is a foundation for building Retrieval-Augmented Generation (RAG) pipelines. It provides accurate answers based on your documents and offers a simple API for indexing and querying document collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60671cf7-976a-4466-b1be-066ba8353ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Setup and Prerequisites\n",
    "\n",
    "# First, let's set up our environment:\n",
    "# Ensure you have conda installed in your system before running this cell."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8346d833-8d51-45d6-9c78-c0c5e193af8f",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "# Let's set up the configuration:\n",
    "\n",
    "# !cp .env.example .env\n",
    "\n",
    "# Edit .env file with your settings\n",
    "# You can use !echo \"KEY=VALUE\" >> .env to add settings\n",
    "\n",
    "# Start Redis and Qdrant services\n",
    "!docker-compose -f docker/docker-compose.yml down\n",
    "!docker-compose -f docker/docker-compose.yml up -d redis qdrant\n",
    "\n",
    "# Setup LLM (Ollama example)\n",
    "!docker-compose -f docker/docker-compose-ollama.yml up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd15aa-9177-432f-883a-e145dc705a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Configuring and Running the Pipeline\n",
    "\n",
    "# First, let's update some important configuration settings\n",
    "from llamasearch.settings import config\n",
    "from llamasearch.pipeline import Pipeline, setup_global_embed_model\n",
    "\n",
    "# Override important settings\n",
    "config.application.data_path = \"./data/test_docs/\"  # Path to your documents\n",
    "config.vector_store_config.collection_name = \"my_test_collection\"\n",
    "# config.embedding.model = \"sentence-transformers/all-MiniLM-L6-v2\"  # A smaller, faster model for testing\n",
    "# config.vector_store_config.vector_size = 384 # Update vector dims to match the embed model dims\n",
    "\n",
    "# Set up the global embedding model\n",
    "global_embed_model = setup_global_embed_model(config)\n",
    "\n",
    "# Now, let's run the pipeline:\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "async def run_pipeline():\n",
    "    tenant_id = \"test_tenant\"\n",
    "    pipeline = Pipeline(config, tenant_id, global_embed_model)\n",
    "    await pipeline.setup()\n",
    "    return pipeline\n",
    "\n",
    "# Use this for Jupyter notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "pipeline = asyncio.get_event_loop().run_until_complete(run_pipeline())\n",
    "\n",
    "print(\"Pipeline setup complete. Ready for queries!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef00c31-ce18-433e-b3b4-fe93a72bb568",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Querying and Results\n",
    "\n",
    "# Let's perform a query:\n",
    "\n",
    "async def perform_query(pipeline, query):\n",
    "    response = await pipeline.perform_query_async(query)\n",
    "    return response\n",
    "\n",
    "query = \"How does attention mechanism work in transformer architecture?\"\n",
    "response = asyncio.run(perform_query(pipeline, query))\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdedb44-6840-4240-ac02-53762cb51e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print the context\n",
    "pipeline.pretty_print_context(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b423aa-306c-4729-ad86-8b4793a1bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "asyncio.run(pipeline.cleanup())\n",
    "\n",
    "print(\"Pipeline cleaned up.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
